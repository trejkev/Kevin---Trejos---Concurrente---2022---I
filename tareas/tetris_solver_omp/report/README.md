# Results report

Here will be listed all the results analysis from the comparison of the different results obtained from serial, concurrent with pthreads, and concurrent with OpenMP solutions. The computer used is a Lenovo Ideapad 3 running an AMD Ryzen 7 processor, 8 cores total at 100% availability (no virtual machine).

## Errors disclaimer

When the programming from pthreads approach was adapted to OpenMP usage, some issues were found, these will be listed and analyzed below, by the hand of Valgrind tsan sanitizer complaining logs.

### OpenMP dificulties

Private data usage for OpenMP is different to the approach taken with pthreads approach, everything that is passed to parallel pragmas as private must be reassigned, and this is because of the internal directives of OpenMP. So, to avoid dealing with this issue, private data inserted to parallel pragmas was avoided, instead private data records were created, used, and deleted into the parallel pragma, and the values from these records needed out of this pragma were passed through a shared variable thread safe (each thread has its own memory portion, so that all of them can write the variable at a time without any problem).

### Sanitizer results

All the four sanitizers were run:

1. asan: Memory address bugs, like memory leaks and use after free.
2. msan: Uninitialized memory usage.
3. tsan: Thread sanitizer for data race detection.
4. ubsan: Undefined behavior sanitizer to check conditions that may lead to undefined behaviors.
5. memcheck: General memory error detector. Like accessing memory out of bounds, use after free, and so on.

Inbetween all of these sanitizers, only tsan complained about the code, reason why only this sanitizer will be analized as part of the sanitizer results.

#### Thread Sanitizer (tsan) errors

In general, it can be told that there are no real data races, all the shared variables are thread safe, and also there is no possibility of a variable being read while written because of this same reason, a thread safe variable is partitioned per thread, so they are either reading or writing, not both.

The first error is shown in the figure 1 and not all the time shows up, shows up in different sections of the code, and even complains with different threads. It can be seen, since it does not return any code line, and because of the difectives telling are being under conflict (this code does not use any pthreads feature) are not directly being used by the code, it is assumed that an internal layer of OpenMP is using them and are getting conflicts with my code. To evidence that there is not a single use of critical, pthreads or so, see the figure 3. Real second error is reporting the same logs but with different threads.

<p align="center">
<img width="1000" src="https://user-images.githubusercontent.com/18760154/175224215-3d9b46f3-f1a0-466d-9961-7ec2dcd4bd37.png">
</p>
<p align="center">Figure 1. First error from tsan, race condition.</p>


The second error is shown in the figure 2. It comes right after elapsed time calculation, which is out of the parallel zone. However, getting deeper in analysis of the warning, it can be seen that the complain of the sanitizer is because there is a read at line 118 that could be crashing with a write of the same variable at line 88.

<p align="center">
<img width="1000" src="https://user-images.githubusercontent.com/18760154/175184746-e3ce4c0e-33c3-44f2-bdef-41842309cc1c.png">
</p>
<p align="center">Figure 2. Second error from tsan, race condition.</p>

Since pragma parallel internally implements a joint feature, so that no thread can continue until all threads get to the end of the parallel section, which is between lines 71 and 100, there is no way in which the write at line 88 can affect the read at line 118. It can be seen in the figure 3.

<p align="center">
<img width="600" src="https://user-images.githubusercontent.com/18760154/175185976-733b4796-2383-4239-bbb1-1a447102f448.png">
</p>
<p align="center">Figure 3. Code sample for demonstration of no rw data race in warning 2.</p>

The third, and last warning, is about a data race being generated by the memory release instruction for all_private_data record, and the writing into the parallel method of this record. Since free command is executed many lines after parallel section is executed, and since OpenMP internally implements a threads joint condition to stop the threads until all the threads complete the parallel method, it is impossible for this condition to happen. See for reference of the warning the figure 4, and the figure 5 for reference of the code.

<p align="center">
<img width="1000" src="https://user-images.githubusercontent.com/18760154/175228324-eb0fa763-ed39-4c8e-a315-74a00994de44.png">
</p>
<p align="center">Figure 4. Third error from tsan, race condition.</p>

<p align="center">
<img width="600" src="https://user-images.githubusercontent.com/18760154/175229017-6dd70df0-6216-44d1-9fe5-912e311648f9.png">
</p>
<p align="center">Figure 5. Code sample for demonstration of no rw data race in warning 3.</p>

## Data used

The data used corresponds to 15 different runs performed for the concurrent methodologies, and 3 for the serial methodology. For real there are only 5 measurements, the case is that 3 replicas were executed to take from each of the replicas the one with the lower elapsed time, so at the end only 5 measurements from each methodology will be taken into consideration.

The tables containing the data are 1 for serial, 2 for pthreads, and 3 for OpenMP, data points to be used are bolded in these tables. Original dataset can be found in this file: https://github.com/trejkev/Kevin-Trejos-Concurrente-2022-I/blob/main/tareas/tetris_solver_omp/report/Results%20data.ods

Table 1. Serial methodology data.
| **Depth** | **Elapsed time (s)** |
|:---------:|:--------------------:|
|     4     |     488.008738379    |
|     4     |  **487.416009964**   |
|     4     |     488.285397118    |

Table 2. Pthreads concurrent methodology.
| **Threads quantity** | **Depth** | **Elapsed time (s)** |
|:--------------------:|:---------:|:--------------------:|
|           1          |     4     |   **418.926235324**  |
|           1          |     4     |     419.603934884    |
|           1          |     4     |     420.158792183    |
|           8          |     4     |   **98.552673194**   |
|           8          |     4     |     99.336651215     |
|           8          |     4     |     99.496066489     |
|           4          |     4     |   **135.392164549**  |
|           4          |     4     |     137.881220136    |
|           4          |     4     |     135.843169263    |
|          16          |     4     |     103.053449023    |
|          16          |     4     |     103.207212195    |
|          16          |     4     |   **102.651160195**  |
|          20          |     4     |     94.354946804     |
|          20          |     4     |     93.931389835     |
|          20          |     4     |   **93.693358154**   |

Table 3. OpenMP concurrent methodology.
| **Threads quantity** | **Depth** | **Elapsed time (s)** |
|:--------------------:|:---------:|:--------------------:|
|           1          |     4     |     382.892311614    |
|           1          |     4     |   **382.602331637**  |
|           1          |     4     |     384.525122109    |
|           8          |     4     |   **98.033008987**   |
|           8          |     4     |     99.279209856     |
|           8          |     4     |     98.953925237     |
|           4          |     4     |     135.579784241    |
|           4          |     4     |   **134.944991575**  |
|           4          |     4     |     135.555215921    |
|          16          |     4     |   **102.465610064**  |
|          16          |     4     |     104.014504887    |
|          16          |     4     |     103.687993405    |
|          20          |     4     |   **93.293360201**   |
|          20          |     4     |      93.4250604      |
|          20          |     4     |      93.44776767     |

## Optimizations comparison

In first place, the two concurrent methodologies were compared to serial methodology, in general terms the code is the same, however, by dividing to conquer, the task of finding the best solution for the game could converge faster. To demonstrate this theory, all the different concurrent scenarios using +1 thread were compared to the serial solution, as a result, a speedup from 3.5 to 5.3 approximately was obtained, it can be confirmed in figure 6. Serial scenario of each of the concurrent solutions were excluded, since their comparison is senseless for the analysis.

<p align="center">
<img width="600" src="https://user-images.githubusercontent.com/18760154/175238656-0716c633-6da8-4c2c-bad5-578edcf08130.png">
</p>
<p align="center">Figure 6. Speedup of concurrent solutions vs serial solution.</p>

After making this metric work, a question that arises is, is higher speedup meaning better resourses usage? And the answer is: not necessarily. For this reason, efficiency was also measured, to check how many threads give the best efficiency, and as shown in figure 7, surprisingly the scenario with lower threads is the one with the best efficiency and it gets lower when increasing the amount of threads, this is because the growth in the speedup happens to be lower, compared to the increase in the threads quantity, so at the end speedup is unable to justify the increase in the threads number.

<p align="center">
<img width="600" src="https://user-images.githubusercontent.com/18760154/175241421-41e3ad89-d047-4ff3-b278-d6067666819e.png">
</p>
<p align="center">Figure 6. Efficiency of concurrent solutions.</p>

When comparing the times obtained from each of the approaches, it can be seen that OpenMP gives a lower elapsed time than Pthreads (but really close one eachother for every sample), and that these both give a a lower elapsed time than serial approach. Serial to concurrent is an obvious explanation, but OpenMP to Pthreads could not be the case, however, short explanation is that OpenMP is a library developed to easily deal with concurrent solutions, and probably it was optimized at the same time, reason why times obtained with OpenMP are slightly better than times obtained with Pthreads approach developed from zero, however, differences are in the order of ms, thus this is not conclusive, bet that even if performing a hypothesis test with a low confidence level.
